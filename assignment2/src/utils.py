import os
import json
from collections import Counter

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from path import *


def merge_config(common_config, arch_config):
    """
    åˆå¹¶é€šç”¨é…ç½®å’Œæ¶æ„é…ç½®ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„å­—å…¸
    """
    config = common_config.copy()
    config.update(arch_config)
    return config

def split_dataset(source_file, train_file, valid_file, valid_lines=1000):
    print(f"Reading source file: {source_file} ...")
    
    if not os.path.exists(source_file):
        print(f"Error: can't find source file {source_file}")
        return

    with open(source_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    total_lines = len(lines)
    print(f"Total lines: {total_lines}")
    
    if total_lines <= valid_lines:
        raise ValueError("The amount of data is too small to create a validation set.")

    # åˆ‡åˆ†æ•°æ®
    train_data = lines[:-valid_lines]
    valid_data = lines[-valid_lines:]
    
    # å†™å…¥è®­ç»ƒé›†
    with open(train_file, 'w', encoding='utf-8') as f:
        f.writelines(train_data)
    print(f"Save train data to: {train_file} (Number of lines: {len(train_data)})")
    
    # å†™å…¥éªŒè¯é›†
    with open(valid_file, 'w', encoding='utf-8') as f:
        f.writelines(valid_data)
    print(f"Save valid data to: {valid_file} (Number of lines: {len(valid_data)})")

class RealTextDataset(Dataset):
    def __init__(self, file_path, vocab=None, seq_len=20, is_train=True):
        self.seq_len = seq_len
        
        # 1. è¯»å–å¹¶æ¸…æ´—æ–‡æœ¬
        with open(file_path, 'r', encoding='utf-8') as f:
            # ç®€å•æ¸…æ´—ï¼šå»é™¤æ¢è¡Œç¬¦ï¼Œå°†æ‰€æœ‰è¡Œæ‹¼æ¥æˆä¸€ä¸ªé•¿å­—ç¬¦ä¸²
            # å¯¹äºè¯­è¨€æ¨¡å‹ï¼Œé€šå¸¸æˆ‘ä»¬å°†æ•´ä¸ªè¯­æ–™è§†ä¸ºä¸€ä¸ªé•¿æµ
            text = f.read().replace('\n', '')
        
        self.data_chars = list(text) # å°†å­—ç¬¦ä¸²è½¬ä¸ºå­—ç¬¦åˆ—è¡¨ ['ä»Š', 'å¤©', ...]
        total_chars = len(self.data_chars)
        print(f"Loaded {file_path}: {total_chars} characters.")

        # 2. æ„å»ºè¯è¡¨ (å¦‚æœæ˜¯è®­ç»ƒé›†)
        if vocab is not None:
            print("Using provided vocabulary.")
            self.token2idx = vocab
        elif is_train:
            print("Building vocabulary from scratch...")
            # ç»Ÿè®¡è¯é¢‘ï¼Œæ„å»ºè¯è¡¨
            vocab_counter = Counter(self.data_chars)
            vocab_list = sorted(vocab_counter, key=vocab_counter.get, reverse=True)
            self.token2idx = {char: idx+1 for idx, char in enumerate(vocab_list)}
            self.token2idx['<unk>'] = 0
        else:
            raise ValueError("Validation set must use training vocab (vocab cannot be None)!")
        
        self.idx2token = {idx: char for char, idx in self.token2idx.items()}
        self.vocab_size = len(self.token2idx)
        
        # 3. å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•
        self.data_ids = [self.token2idx.get(char, self.token2idx['<unk>']) for char in self.data_chars]
        self.data_ids = torch.tensor(self.data_ids, dtype=torch.long)

    def __len__(self):
        # æ•°æ®é‡ = æ€»é•¿åº¦ - åºåˆ—é•¿åº¦
        return len(self.data_ids) - self.seq_len

    def __getitem__(self, idx):
        # è¾“å…¥: text[i : i+seq_len]
        # ç›®æ ‡: text[i+1 : i+seq_len+1]
        src = self.data_ids[idx : idx + self.seq_len]
        trg = self.data_ids[idx + 1 : idx + self.seq_len + 1]
        return src, trg

def save_vocab(vocab, path):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(vocab, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Vocabulary saved to {path}")

def load_vocab(path):
    with open(path, 'r', encoding='utf-8') as f:
        vocab = json.load(f)
    print(f"ğŸ“– Vocabulary loaded from {path}")
    return vocab

def get_dataloaders(config):
    vocab = None
    if os.path.exists(VOCAB_FILE):
        print(f"Found saved vocabulary at {VOCAB_FILE}")
        vocab = load_vocab(VOCAB_FILE)

    print("Processing Training Data...")
    train_ds = RealTextDataset(TRAIN_FILE, vocab=vocab, seq_len=config['seq_len'], is_train=True)
    
    if vocab is None:
        save_vocab(train_ds.token2idx, VOCAB_FILE)

    # æ›´æ–° config ä¸­çš„ vocab_sizeï¼Œå› ä¸ºæ˜¯æ ¹æ®æ•°æ®åŠ¨æ€ç”Ÿæˆçš„
    config['vocab_size'] = train_ds.vocab_size
    print(f"Vocab Size: {config['vocab_size']}")
    
    print("Processing Validation Data...")
    # æ³¨æ„ï¼šéªŒè¯é›†ä¼ å…¥ train_ds.token2idx
    val_ds = RealTextDataset(VALID_FILE, vocab=train_ds.token2idx, seq_len=config['seq_len'], is_train=False)
    
    train_loader = DataLoader(
        train_ds, 
        batch_size=config['batch_size'], 
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )
    val_loader = DataLoader(
        val_ds, 
        batch_size=config['batch_size'], 
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )
    
    return train_loader, val_loader

def save_checkpoint(model, vocab, config, metrics, filename):
    """
    ä¿å­˜æ¨¡å‹æƒé‡ã€è¯è¡¨é…ç½®å’Œè®­ç»ƒæŒ‡æ ‡
    """
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'vocab': vocab,          # ä¿å­˜è¯è¡¨(char->idxæ˜ å°„)
        'config': config,        # ä¿å­˜è¶…å‚æ•°
        'metrics': metrics       # ä¿å­˜è®­ç»ƒæ›²çº¿æ•°æ®
    }
    path = os.path.join(CHECKPOINT_PATH, filename)
    torch.save(checkpoint, path)
    print(f"âœ… æ¨¡å‹ä¸è¯è¡¨å·²ä¿å­˜è‡³: {path}")

def load_checkpoint(filename, model_class, device='cpu'):
    """
    åŠ è½½æ¨¡å‹å’Œè¯è¡¨(ç”±äºæœ‰ wandbï¼Œå…¶å®ä¹Ÿä¸å¤ªéœ€è¦äº†)
    """
    path = os.path.join(CHECKPOINT_PATH, filename)
    if not os.path.exists(path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {path}")
        
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {path} ...")
    checkpoint = torch.load(path, map_location=device)
    
    vocab = checkpoint['vocab']
    config = checkpoint['config']
    
    # æ ¹æ®é…ç½®é‡æ–°åˆå§‹åŒ–æ¨¡å‹ç»“æ„
    if 'RNN' in filename:
        model = model_class(len(vocab), config['embed_dim'], config['hidden_dim'])
    elif 'FNN' in filename:
        model = model_class(len(vocab), config['embed_dim'], config['hidden_dim'], config['seq_len'])
    elif 'Transformer' in filename:
        model = model_class(
            len(vocab),
            config['embed_dim'], 
            config['hidden_dim'], 
            config['num_heads'],
            config['layers'],
            config['dropout']
        )
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    return model, vocab, checkpoint['metrics']

if __name__ == "__main__":
    split_dataset(SOURCE_FILE, TRAIN_FILE, VALID_FILE)