import torch
from model import *
from config import *
from utils import *
from path import *


def generate_text(model, start_text, vocab, idx2token, config, max_new_tokens=50, temperature=1.0, top_k=10):
    """
    è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ (Autoregressive Generation)
    å…¼å®¹ Transformer, RNN å’Œ FNN
    """
    model.eval()
    device = config['device']
    
    # 1. é¢„å¤„ç†è¾“å…¥
    input_ids = [vocab.get(c, vocab['<unk>']) for c in start_text]
    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device) # [1, len]
    
    generated_text = start_text
    
    print(f"ğŸ“– ç”Ÿæˆä¸­ [{type(model).__name__}]: {start_text}", end="", flush=True)
    
    # è·å– Padding çš„ç´¢å¼• (é€šå¸¸ <unk> æ˜¯ 0)
    pad_idx = vocab.get('<unk>', 0)

    with torch.no_grad():
        for _ in range(max_new_tokens):
            # A. æˆªæ–­é€»è¾‘ (Transformer/RNN/FNN éƒ½éœ€è¦å¤„ç†è¿‡é•¿åºåˆ—)
            if input_tensor.size(1) > config['seq_len']:
                cond = input_tensor[:, -config['seq_len']:]
            else:
                cond = input_tensor
                
            # B. å¡«å……é€»è¾‘ (ä¸“ä¸º FNN è®¾è®¡)
            # FNN å¼ºåˆ¶è¦æ±‚è¾“å…¥é•¿åº¦ç­‰äº seq_lenï¼Œå¦åˆ™ fcå±‚ ç»´åº¦å¯¹ä¸ä¸Š
            if type(model).__name__ == 'FNN_LM' and cond.size(1) < config['seq_len']:
                pad_len = config['seq_len'] - cond.size(1)
                pad_tensor = torch.full((1, pad_len), pad_idx, dtype=torch.long, device=device)
                cond = torch.cat((pad_tensor, cond), dim=1) # [pad, context]

            # C. å‰å‘ä¼ æ’­
            logits = model(cond)
            
            # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
            logits = logits[:, -1, :] / temperature
            
            # Top-K é‡‡æ ·
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')
            
            # è®¡ç®—æ¦‚ç‡å¹¶é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            next_token_idx = torch.multinomial(probs, num_samples=1)
            
            # æ‹¼æ¥
            input_tensor = torch.cat((input_tensor, next_token_idx), dim=1)
            
            # è§£ç 
            char = idx2token.get(next_token_idx.item(), '<unk>')
            generated_text += char
            print(char, end="", flush=True)
            
    print("\n")
    return generated_text

def test_model(model_name, start_text="ä»Šå¤©å¤©æ°”"):
    print(f"\n>>> Testing {model_name}...")
    
    # 1. å‡†å¤‡é…ç½®
    if model_name not in MODEL_ARCH_CONFIGS:
        print(f"Unknown model: {model_name}")
        return

    # åŠ è½½è¯è¡¨
    vocab = load_vocab(VOCAB_FILE)
    idx2token = {v: k for k, v in vocab.items()}
    vocab_size = len(vocab)
    
    # åˆå¹¶é…ç½®
    config = merge_config(TRAIN_CONFIG, MODEL_ARCH_CONFIGS[model_name])
    config['vocab_size'] = vocab_size
    
    # 2. åˆå§‹åŒ–æ¨¡å‹ç»“æ„
    device = config['device']
    if model_name == 'FNN':
        model = FNN_LM(vocab_size, config['embed_dim'], config['hidden_dim'], config['seq_len'])
    elif model_name == 'RNN':
        model = RNN_LM(vocab_size, config['embed_dim'], config['hidden_dim'])
    elif model_name == 'Transformer':
        model = Transformer_LM(vocab_size, config['embed_dim'], config['hidden_dim'], 
                               config['num_heads'], config['layers'], config['dropout'])
    
    # 3. åŠ è½½æƒé‡
    # æ³¨æ„æ–‡ä»¶åè¦å’Œä½  train_and_eval.py é‡Œä¿å­˜çš„ä¸€è‡´ï¼Œé€šå¸¸æ˜¯ "FNN.pth" ç­‰
    ckpt_path = os.path.join(CHECKPOINT_PATH, f"{model_name}.pth")
    if not os.path.exists(ckpt_path):
        print(f"Checkpoint not found: {ckpt_path}")
        return
        
    print(f"Loading weights from {ckpt_path}")
    checkpoint = torch.load(ckpt_path, map_location=device)
    
    # å¤„ç† DataParallel ä¿å­˜æ—¶å¸¦æœ‰çš„ 'module.' å‰ç¼€
    state_dict = checkpoint['model_state_dict']
    new_state_dict = {}
    for k, v in state_dict.items():
        name = k[7:] if k.startswith('module.') else k
        new_state_dict[name] = v
        
    model.load_state_dict(new_state_dict)
    model.to(device)
    model.eval()
    
    # 4. ç”Ÿæˆæ–‡æœ¬
    generate_text(model, start_text, vocab, idx2token, config, max_new_tokens=20)

if __name__ == "__main__":
    test_model("FNN", "ä¹Ÿå°±æ˜¯")
    test_model("RNN", "ä¹Ÿå°±æ˜¯")
    test_model("Transformer", "ä¹Ÿå°±æ˜¯")